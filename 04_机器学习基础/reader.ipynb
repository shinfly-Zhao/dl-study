me="wHDjs"></a>
# 4.1 机器学习的四个分支
<a name="xSQaV"></a>
## 4.1.1 监督学习
监督学习是目前最常见的机器学习类型。给定一组样本（通常由人工标注），它可以学会将
输入数据映射到已知目标［也叫标注（annotation）］。一
般来说，近年来广受关注的深度学习应用几乎都属于监督学习，比如光学字符识别、语音识别、
图像分类和语言翻译。
虽然监督学习主要包括分类和回归，但还有更多的奇特变体，主要包括如下几种。

- 序列生成（sequence generation）。给定一张图像，预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题，比如反复预测序列中的单词或标记。
-  语法树预测（syntax tree prediction）。给定一个句子，预测其分解生成的语法树。
-  目标检测（object detection）。给定一张图像，在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题（给定多个候选边界框，对每个框内的目标进行分类）或分类与回归联合问题（用向量回归来预测边界框的坐标）。
-  图像分割（image segmentation）。给定一张图像，在特定物体上画一个像素级的掩模（mask）。
<a name="zcs7i"></a>
## 4.1.2 无监督学习
无监督学习是指在没有目标的情况下寻找输入数据的有趣变换，**其目的在于数据可视化、
数据压缩、数据去噪或更好地理解数据中的相关性**。无监督学习是数据分析的必备技能，在解决监督学习问题之前，为了更好地了解数据集，它通常是一个必要步骤。降维（dimensionality
reduction）和聚类（clustering）都是众所周知的无监督学习方法。

<a name="Cn9Vw"></a>
## 4.1.3 自监督学习 
自监督学习是监督学习的一个特例，它与众不同，值得单独归为一类。自监督学习是没有人工标注的标签的监督学习，可以将它看作没有人类参与的监督学习。标签仍然存在（因为总要有什么东西来监督学习过程），但它们是从输入数据中生成的，通常是使用启发式算法生成的。举个例子，自编码器（autoencoder）是有名的自监督学习的例子，其生成的目标就是未经修改的输入。同样，**给定视频中过去的帧来预测下一帧，或者给定文本中前面的词来预测下一个词，
都是自监督学习的例子**［这两个例子也属于时序监督学习（temporally supervised learning），即用未来的输入数据作为监督］。注意，监督学习、自监督学习和无监督学习之间的区别有时很模糊，
这三个类别更像是没有明确界限的连续体。自监督学习可以被重新解释为监督学习或无监督学习，这取决于关注的是学习机制还是应用场景。
<a name="ayE3g"></a>
# 4.2 常用术语

- 样本（sample）或输入（input）：进入模型的数据点。 
- 预测（prediction）或输出（output）：从模型出来的结果。 
- 目标（target）：真实值。对于外部数据源，理想情况下，模型应该能够预测出目标。 
- 预测误差（prediction error）或损失值（loss value）：模型预测与目标之间的距离。
- 类别（class）：分类问题中供选择的一组标签。例如，对猫狗图像进行分类时，“狗”
和“猫”就是两个类别。
- 标签（label）：分类问题中类别标注的具体例子。比如，如果 1234 号图像被标注为
包含类别“狗”，那么“狗”就是 1234 号图像的标签。 
- 真值（ground-truth）或标注（annotation）：数据集的所有目标，通常由人工收集。 
- 二分类（binary classification）：一种分类任务，每个输入样本都应被划分到两个互斥的类别中。 
- 多分类（multiclass classification）：一种分类任务，每个输入样本都应被划分到两个以上的类别中，比如手写数字分类。 
- 多标签分类（multilabel classification）：一种分类任务，每个输入样本都可以分配多
个标签。举个例子，如果一幅图像里可能既有猫又有狗，那么应该同时标注“猫”
标签和“狗”标签。每幅图像的标签个数通常是可变的。
- **标量回归**（scalar regression）：目标是**连续标量值**的任务。预测房价就是一个很好的例子，不同的目标价格形成一个连续的空间。
- ** 向量回归**（vector regression）：目标是**一组连续值**（比如一个连续向量）的任务。如果对多个值（比如图像边界框的坐标）进行回归，那就是向量回归。 
- 小批量（mini-batch）或批量（batch）：模型同时处理的一小部分样本（样本数通常
为 8~128）。样本数通常取 2 的幂，这样便于GPU上的内存分配。训练时，小批量用来为模型权重计算一次梯度下降更新。
<a name="7UfVB"></a>
# 4.3 评估机器学习模型
机器学习的目的是得到可以泛化（generalize）的模型，即在前所未见的数据上表现很好的模型，而过拟合则是核心难点。只能控制可以观察的事情，所以能够可靠地衡量模型的泛化能力非常重要。
<a name="HIOZk"></a>
## 4.3.1 训练集 验证集 测试集
评估模型的重点是将数据划分为三个集合：训练集、验证集和测试集。在训练数据上训练模型，在验证数据上评估模型。一旦找到了最佳参数，就在测试数据上最后测试一次。<br />如果数据只分为训练数据集和测试数据集，那么模型可能会在测试数据集上过拟合，因为我们是通过测试数据集来调整模型，所以这样下来模型的泛化性可能较差。<br />但是，如果数据集本身的样本数较少，可以使用如下的方法

- 简单的留出验证
- K 折验证
- 以及带有打乱数据的重复 K 折验证。
<a name="DWQFU"></a>
## 4.3.2 评估模型的重要事项 
选择模型评估方法时，需要注意以下几点。 

- 数据代表性（data representativeness）。你希望训练集和测试集都能够代表当前数据。例如，你想要对数字图像进行分类，而图像样本是按类别排序的，如果你将前 80% 作为训练集，剩余20%作为测试集，那么会导致训练集中只包含类别 0~7，而测试集中只包含
类别 8~9。这个错误看起来很可笑，却很常见。**因此，在将数据划分为训练集和测试集之前，通常应该随机打乱数据。**
- 时间箭头（the arrow of time）。如果想要根据过去预测未来（比如明天的天气、股票走势
等），那么在划分数据前你不应该随机打乱数据，因为这么做会造成时间泄露（temporal
leak）：你的模型将在未来数据上得到有效训练。在这种情况下，你应该始终确保测试集中所有数据的时间都晚于训练集数据。
- **数据冗余（redundancy in your data）**。如果数据中的某些数据点出现了两次，那么打乱数据并划分成训练集和验证集会导致训练集和验证集之间的数据冗余。从效果上来看，你是在部分训练数据上评估模型，这是极其糟糕的！一
定要确保训练集和验证集之间没有交集。
<a name="hNa1x"></a>
# 4.4 数据预处理、特征工程和特征学习
在深入研究模型开发之前，还必须解决另一个重要问题：将数据输入神经网络之前，如何准备输入数据和目标？许多数据预处理方法和特征工程技术都是和特定领域相关的（比如只和文本数据或图像数据相关），一下所述是所有数据领域通用的基本方法。
<a name="Ql7f0"></a>
## 4.4.1  神经网络的数据预处理
数据预处理的目的是使原始数据更适于用神经网络处理，包括向量化、标准化、处理缺失
值和特征提取。

1. 向量化<br />
神经网络的所有输入和目标都必须是浮点数张量（在特定情况下可以是整数张量）。无论处理什么数据（声音、图像还是文本），都必须首先将其转换为张量，这一步叫作数据向量化（data vectorization）。
1. 值标准化<br />
在手写数字分类的例子中，开始时图像数据被编码为 0~255 范围内的整数，表示灰度值。将这一数据输入网络之前，需要将其转换为 float32 格式并除以 255，这样就得到 0~1 范围内的浮点数。同样，预测房价时，开始时特征有各种不同的取值范围，有些特征是较小的浮点数，有些特征是相对较大的整数。将这一数据输入网络之前，需要对每个特征分别做标准化，使其均值为 0、标准差为 1。一般来说，将取值相对较大的数据（比如多位整数，比网络权重的初始值大很多）或异质数据（heterogeneous data，比如数据的一个特征在 0~1 范围内另一个特征在 100~200 范围内）输入到神经网络中是不安全的。这么做可能导致较大的梯度更新，进而导致网络无法收敛。为了让网络的学习变得更容易，输入数据应该具有以下特征。
- 取值较小：大部分值都应该在 0~1 范围内。
- 同质性（homogenous）：所有特征的取值都应该在大致相同的范围内。

3  处理缺失值<br />数据中有时可能会有缺失值。例如在房价的例子中，第一个特征（数据中索引编号为
0 的列）是人均犯罪率。如果不是所有样本都具有这个特征的话，怎么办？那样训练数据或测试数据将会有缺失值。一般来说，对于神经网络，将缺失值设置为0是安全的，只要0不是一个有意义的值。网络能够从数据中学到 0 意味着缺失数据，并且会忽略这个值。注意，如果测试数据中可能有缺失值，而网络是在没有缺失值的数据上训练的，那么网络不可能学会忽略缺失值。在这种情况下，应该人为生成一些有缺失项的训练样本：多次复制一些训练样本，然后删除测试数据中可能缺失的某些特征。
<a name="bUgRG"></a>
## 4.4.2 特征工程 
特征工程（feature engineering）是指将数据输入模型之前，利用你自己关于数据和机器学习算法（这里指神经网络）的知识对数据进行硬编码的变换（不是模型学到的），以改善模型的效果。多数情况下，一个机器学习模型无法从完全任意的数据中进行学习。呈现给模型的数据应该便于模型进行学习。我们来看一个直观的例子。假设你想开发一个模型，输入一个时钟图像，模型能够输出对应的时间（见下图）。<br />![01.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567263628891-9a140cb7-2430-4c1e-840a-399417175b66.png#align=left&display=inline&height=167&name=01.png&originHeight=311&originWidth=758&size=54999&status=done&width=406)<br />如果选择用图像的原始像素作为输入数据，那么这个机器学习问题将非常困难。需要用卷积神经网络来解决这个问题，而且还需要花费大量的计算资源来训练网络。
但如果从更高的层次理解了这个问题（你知道人们怎么看时钟上的时间），那么可以为机器学习算法找到更好的输入特征，比如找到时钟指针对应的黑色像素并输出每个指针尖的 (x, y) 坐标，这很简单。然后，一个简单的机器学习算法就可以学会这些坐标与时间的对应关系。还可以进一步思考：进行坐标变换，将 (x, y) 坐标转换为相对于图像中心的极坐标。这样输入就变成了每个时钟指针的角度theta。现在的特征使问题变得非常简单，根本不需要机器学习，因为简单的舍入运算和字典查找就足以给出大致的时间。
这就是特征工程的本质：用更简单的方式表述问题，从而使问题变得更容易。它通常需要深入理解问题。深度学习出现之前，特征工程曾经非常重要，因为经典的浅层算法没有足够大的假设空间来自己学习有用的表示。将数据呈现给算法的方式对解决问题至关重要。例如，卷积神经网络在MNIST数字分类问题上取得成功之前，其解决方法通常是基于硬编码的特征，比如数字图像中的圆圈个数、图像中每个数字的高度、像素值的直方图等。幸运的是，对于现代深度学习，大部分特征工程都是不需要的，因为神经网络能够从原始数据中自动提取有用的特征。这是否意味着，只要使用深度神经网络，就无须担心特征工程呢？
并不是这样，原因有两点。

- 良好的特征仍然可以让你用更少的资源更优雅地解决问题。例如，使用卷积神经网络来读取钟面上的时间是非常可笑的。
- 良好的特征可以让你用更少的数据解决问题。深度学习模型自主学习特征的能力依赖于大量的训练数据。如果只有很少的样本，那么特征的信息价值就变得非常重要。
<a name="vBZqe"></a>
# 4.5 过拟合与欠拟合
<a name="Ss8ti"></a>
## 4.5.1 减小网络大小
防止过拟合的最简单的方法就是**减小模型大小**，即减少模型中可学习参数的个数（这由层数和每层的单元个数决定）。在深度学习中，模型中可学习参数的个数通常被称为**模型的容量
（capacity）**。直观上来看，参数更多的模型拥有更大的记忆容量（memorization capacity），因此能够在训练样本和目标之间轻松地学会完美的字典式映射，**这种映射没有任何泛化能力**。例如，拥有500000个二进制参数的模型，能够轻松学会 MNIST 训练集中所有数字对应的类别——只需让50000个数字每个都对应 10 个二进制参数。但这种模型对于新数字样本的分类毫无用处。始终牢记：深度学习模型通常都很**擅长拟合训练数据，但真正的挑战在于泛化，而不是拟合**。
与此相反，如果网络的记忆资源有限，则无法轻松学会这种映射。因此，为了让损失最小化，
网络必须学会对目标具有很强预测能力的压缩表示，这也正是我们感兴趣的数据表示。同时请记住，你使用的模型应该具有足够多的参数，以防欠拟合，即模型应避免记忆资源不足。在容量过大与容量不足之间要找到一个折中。不幸的是，没有一个魔法公式能够确定最佳层数或每层的最佳大小。必须评估一系列不同的网络架构（当然是在验证集上评估，而不是在测试集上），以便为数据找到最佳的模型大小。
要找到合适的模型大小，一般的工作流程是开始时选择相对较少的层和参数，然后逐渐增加层
的大小或增加新层，直到这种增加对验证损失的影响变得很小。<br />下图比较了原始网络与更小网络的**验证损失**。圆点是更小网络的验证损失值，十字是原始网络的验证损失值（**更小的验证损失对应更好的模型**）。
**图表示模型容量对验证损失的影响**：换用更小的网络，更小的网络开始过拟合的时间要晚于参考网络（前者6轮后开始过拟合，而后者4轮后开始），而且开始过拟合之后，它的性能变差的速度也更慢。
现在，再向这个基准中添加一个容量更大的网络<br />![02.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567301924049-1a69edba-6003-4cea-a02d-ed10503137fb.png#align=left&display=inline&height=203&name=02.png&originHeight=332&originWidth=854&size=54152&status=done&width=523)<br />下图显示了更大的网络与参考网络的性能对比。圆点是更大网络的验证损失值，十字是原始网络的验证损失值。<br />![03.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567302029860-1b34fb0f-5c5f-47fe-adfc-d13cc21a2a92.png#align=left&display=inline&height=187&name=03.png&originHeight=323&originWidth=848&size=56369&status=done&width=491)<br />下图给出了这两个网络的训练损失。可以看出更大网络的训练损失很快就接近于零。
网络的容量越大，它拟合训练数据（即得到很小的训练损失）的速度就越快，但也更容易过拟合
（导致训练损失和验证损失有很大差异）。<br />
<br />![03.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567302792377-b4aeb8f6-f441-4336-bb92-ef2c175d7e07.png#align=left&display=inline&height=191&name=03.png&originHeight=322&originWidth=846&size=50172&status=done&width=503)
<a name="869lq"></a>
## 4.5.2 添加权重正则化    
给定一些训练数据和一种网络架构，很多组权重值（即很多模型）都可以解释这些数据。简单模型比复杂模型更不容易过拟合。<br />简单模型（simple model）是指参数值分布的熵更小的模型（或参数更少的模型）。因此，一种常见的降低过拟合的方法就是**强制让模型权重只能取较小的值 从而限制模型的复杂度，这使得权重值的分布更加规则**（regular）。这种方法叫作**权重正则化**
（weight regularization），其实现方法是向网络损失函数中添加与较大权重值相关的成本（cost）。
这个成本有两种形式。

- L1 正则化（L1 regularization）：添加的成本与权重系数的绝对值［权重的 L1 范数（norm）］
成正比
- L2 正则化（L2 regularization）：添加的成本与权重系数的平方（权重的 L2 范数）成正比。
神经网络的 L2 正则化也叫权重衰减（weight decay）

![](https://cdn.nlark.com/yuque/__latex/d7f22cf8f728aeab96efe4e7076631f9.svg#card=math&code=L1%20%3D%20%5Calpha%5Cbegin%7Bequation%2A%7D%0A%0A%20%2A%20%5Csum_%7Bi%3D1%7D%5Em%7CW_i%0A%7C%0A%5Cend%7Bequation%2A%7D%20%20%0A&height=49&width=127)<br />
<br />![](https://cdn.nlark.com/yuque/__latex/70ea659c61d8cc64f62df40929e63cbd.svg#card=math&code=L2%20%3D%20%5Calpha%20%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bequation%2A%7D%0A%0A%20%2A%20%5Csum_%7Bi%3D1%7D%5Em%28W_i%0A%29%5E2%0A%5Cend%7Bequation%2A%7D%20%20%0A&height=49&width=150)
<a name="GzOTJ"></a>
## 4.5.3 添加 dropout 正则化
对某一层使用 dropout，就是在训练过程中随机将该层的一些输出特征舍弃（设置为 0）。假设在训练过程中，某一层对给定输入样本的返回值应该是向量 [0.2, 0.5,
1.3, 0.8, 1.1]。使用dropout后，这个向量会有几个随机的元素变成0，比如[0, 0.5,
1.3, 0, 1.1]。

<a name="0hWb0"></a>
# 4.6 机器学习的通用工作流程

- 定义问题，收集数据集
- 选择衡量成功的指标
- 确定评估
- 准备数据
- 开发比基准更好的模型
- 扩大模型规模：开发过拟合的模型
- 模型正则化与调节超参数

