me="vZu8f"></a>
# 5.1 卷积神经网络
卷积神经网络，也叫 convnet，它是计算机视觉应用几乎都在使用的一种深度学习模型。卷积神经网络应用于图像分类问题，特别是那些训练数据集较小的问题。
<a name="ZqdET"></a>
## 5.1.1 卷积运算
密集连接层和卷积层的根本区别在于，Dense 层从输入特征空间中学到的是全局模式，（比如对于 MNIST 数字，全局模式就是涉及所有像素的模式），而卷积层学到的是局部模式对于图像来说，学到的就是在输入图像的二维小窗口中发现的模式。<br />![01.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567649102139-3612d74d-ab35-4f06-9f0d-cd6d9585ea7c.png#align=left&display=inline&height=186&name=01.png&originHeight=327&originWidth=723&size=19624&status=done&width=411)<br />这个重要特性使卷积神经网络具有以下两个性质。

- 卷积神经网络学到的模式具有**平移不变性（translation invariant）**。卷积神经网络在图像右下角学到某个模式之后，它可以在任何地方识别这个模式，比如左上角。对于密集连接网络来说，如果模式出现在新的位置，它只能重新学习这个模式。这使得卷积神经网络在处理图像时可以高效利用数据（因为视觉世界从根本上具有平移不变性），它只需要更少的训练样本就可以学到具有泛化能力的数据表示。
- 卷积神经网络可以学到**模式的空间层次结构**（spatial hierarchies of patterns），见下图。
第一个卷积层将学习较小的局部模式（比如边缘），第二个卷积层将学习由第一层特征组成的更大的模式，以此类推。**这使得卷积神经网络可以有效地学习越来越复杂、越来越抽象的视觉概念**（因为视觉世界从根本上具有空间层次结构）对于包含两个空间轴（高度和宽度）和一个深度轴（也叫通道轴）的 3D 张量，其卷积也叫特征图（feature map）。对于RGB图像，深度轴的维度大小等于3，因为图像有3个颜色通道：
红色、绿色和蓝色。对于黑白图像（比如 MNIST 数字图像），深度等于 1（表示灰度等级）。卷
积运算从输入特征图中提取图块，并对所有这些图块应用相同的变换，生成输出特征图（output
feature map）。该输出特征图仍是一个 3D 张量，具有宽度和高度，其深度可以任意取值，因为输出深度是层的参数，深度轴的不同通道不再像 RGB 输入那样代表特定颜色，而是代表过滤器
（filter）。过滤器对输入数据的某一方面进行编码，比如，单个过滤器可以从更高层次编码这样
一个概念：“输入中包含一张脸。”

![02.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567649759072-bab59838-a0fb-4863-8c36-ca4b6881cfb3.png#align=left&display=inline&height=174&name=02.png&originHeight=424&originWidth=835&size=54293&status=done&width=342)<br />视觉世界形成了视觉模块的空间层次结构：超局部的边缘组合成局部的对象，<br />
比如眼睛或耳朵，这些局部对象又组合成高级概念，比如“猫”<br />在MNIST示例中，第一个卷积层接收一个大小为 (28, 28, 1) 的特征图，并输出一个大小为 (26, 26, 32) 的特征图，即它在输入上计算 **32 **个过滤器。对于这 32 个输出通道，**每个通道都包含一个26×26的数值网格**，它是过滤器对输入的**响应图**（response map），表示这个过滤器模式在输入中不同位置的响应（见图 5-3）。这也是特征图这一术语的含义：深度轴的每个维度都是一个特征（或过滤器），而2D张量 output[:, :, n] 是这个过滤器在输入上的响应的二维空间图（map）。<br />![03.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567650569178-dc5664b3-8eb3-4c32-854a-47531b32f405.png#align=left&display=inline&height=98&name=03.png&originHeight=241&originWidth=836&size=38179&status=done&width=341)<br />图 5.3 响应图的概念：某个模式在输入中的不同位置是否存在的二维图<br />卷积的工作原理：

1. 2D 

对于二维图像来说 输入（5，5） 过滤器大小 （3,3 ) ** 注意卷积第三通道大小必须和上层输入数据的通道数一致，**卷积过程就是用（3，3）的 过滤器在原数据上依次滑动并于相对应得数据做成绩，最后相加得一次结果。如下过程：<br />

2. 3D

对于3D数据来说 ，使用 过滤器的每个通道与输入数据相应通道做2D卷积过程。<br />
<br />注意，输出的宽度和高度可能与输入的宽度和高度不同。不同的原因可能有两点。

- 边界效应，可以通过对输入特征图进行填充来抵消。
- 使用了步幅（stride）
<a name="OIqh7"></a>
### 5.1.1.1边界效应与填充
假设有一个 5×5 的特征图（共 25 个方块）。其中只有 9 个方块可以作为中心放入一个
3×3 的窗口，这9个方块形成一个 3×3 的网格（见下图）。因此，输出特征图的尺寸是 3×3。
它比输入尺寸小了一点。<br />![04.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567653084076-bcc70d5d-193c-45d5-b4dd-ffaea6bcef81.png#align=left&display=inline&height=191&name=04.png&originHeight=355&originWidth=831&size=9467&status=done&width=448)<br />如果希望输出特征图的空间维度与输入相同，那么可以使用填充（padding）。填充是在输入特征图的每一边添加适当数目的行和列，使得每个输入方块都能作为卷积窗口的中心。对于3×3 的窗口，在左右各添加一列，在上下各添加一行。对于 5×5 的窗口，各添加两行和两列。<br />![05.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567653197548-be43ff29-f576-4cbf-9902-ab3ba7a54dbc.png#align=left&display=inline&height=102&name=05.png&originHeight=245&originWidth=833&size=48621&status=done&width=348)
<a name="4LkM8"></a>
### 5.1.1.2步幅
影响输出尺寸的另一个因素是步幅的概念。目前为止，对卷积的描述都假设卷积窗口的中心方块都是相邻的。但两个连续窗口的距离是卷积的一个参数，叫作步幅，默认值为 1。也可以使用步进卷积（strided convolution），即步幅大于 1 的卷积。如下图所示步幅为2 <br />
<br />![06.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567653360507-c84428d3-ed53-4e03-b6c8-24723cce58c3.png#align=left&display=inline&height=119&name=06.png&originHeight=309&originWidth=841&size=9092&status=done&width=323)<br />

<a name="0cb2ada9"></a>
# 5.2 猫狗分类

<a name="425679f9"></a>
## 5.2.1 数据前期处理

```python
import os, shutil ,sys 
original_dataset_dir = './data/dogs-vs-cats/alldata/' # 原始数据集 
# base_dir = './data/dogs-vs-cats/small_data/'  # 小数据集
# os.mkdir(base_dir)
# 创建训练及相关类别的目录
train_dir_dog = os.path.join(base_dir, 'train',"dogs")
train_dir_cat = os.path.join(base_dir, 'train',"cats")
validation_dog = os.path.join(base_dir, 'validation',"dogs")
validation_cat = os.path.join(base_dir, 'validation',"cats")
test_dog = os.path.join(base_dir, 'test',"dogs")
test_cat = os.path.join(base_dir, 'test',"cats")
# dirlist = [train_dir_dog,train_dir_cat,validation_dog,validation_cat,test_dog,test_cat]
# for dit_data in dirlist:
#     os.makedirs(dit_data) # 多级目录
```

```python
# fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(train_dir_cat, fname)
#     shutil.copyfile(src, dst)

# fnames = ['cat.{}.jpg'.format(i) for i in range(1000,1500)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(validation_cat, fname)
#     shutil.copyfile(src, dst)
    
# fnames = ['cat.{}.jpg'.format(i) for i in range(1500,2000)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(test_cat, fname)
#     shutil.copyfile(src, dst)
```

```python
# fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(train_dir_dog, fname)
#     shutil.copyfile(src, dst)

# fnames = ['dog.{}.jpg'.format(i) for i in range(1000,1500)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(validation_dog, fname)
#     shutil.copyfile(src, dst)
    
# fnames = ['dog.{}.jpg'.format(i) for i in range(1500,2000)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(test_dog, fname)
#     shutil.copyfile(src, dst)
```

<a name="c502d32d"></a>
## 5.2.2 网络搭建

二分类问题，所以网络最后一层是使用 sigmoid 激活的单一单元（大小为1的Dense层）。这个单元将对某个类别的概率进行编码

```python
from keras import layers
from keras import models
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())  # 展开为向量
model.add(layers.Dense(512, activation='relu'))  # 全连结层
model.add(layers.Dense(1, activation='sigmoid'))
model.summary()
```

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_9 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
```

```python
from keras import optimizers
model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(lr=1e-4),metrics=['acc'])
```

<a name="1d069b32"></a>
## 5.2.3 数据预处理

- 读取图像文件。
- 将 JPEG 文件解码为 RGB 像素网格。
- 将这些像素网格转换为浮点数张量。
- 将像素值（0~255 范围内）缩放到 [0, 1] 区间

```python
from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(os.path.join(base_dir, 'train'),target_size=(150, 150),batch_size=20,class_mode='binary')
validation_generator = test_datagen.flow_from_directory( os.path.join(base_dir, 'validation'),target_size=(150, 150),batch_size=20,class_mode='binary')
```

```
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
```

```python
for data_batch, labels_batch in train_generator:
    print('data batch shape:', data_batch.shape)
    print('labels batch shape:', labels_batch.shape)
    break
```

```
data batch shape: (20, 150, 150, 3)
labels batch shape: (20,)
```

<a name="a418a0d1"></a>
## 5.2.4 模型训练

利用生成器，让模型对数据进行拟合。我们将使用 fit_generator 方法来拟合，它<br />在数据生成器上的效果和 fit 相同。它的第一个参数应该是一个 Python 生成器，可以不停地生<br />成输入和目标组成的批量，比如 train_generator。因为数据是不断生成的，所以 Keras 模型<br />要知道每一轮需要从生成器中抽取多少个样本。这是 steps_per_epoch 参数的作用：从生成<br />器中抽取 steps_per_epoch 个批量后（即运行了 steps_per_epoch 次梯度下降），拟合过程<br />将进入下一个轮次。本例中，每个批量包含 20 个样本，所以读取完所有 2000 个样本需要 100<br />个批量。使用 fit_generator 时，你可以传入一个 validation_data 参数，其作用和在 fit 方<br />法中类似。值得注意的是，这个参数可以是一个数据生成器，但也可以是 Numpy 数组组成的元<br />组。如果向 validation_data 传入一个生成器，那么这个生成器应该能够不停地生成验证数<br />据批量，因此你还需要指定 validation_steps 参数，说明需要从验证生成器中抽取多少个批<br />次用于评估。

```python
history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=30,validation_data=validation_generator,validation_steps=50)
model.save('./models/cats_and_dogs_small_1.h5')
```

```
W0906 11:47:33.155051 140165159728960 deprecation_wrapper.py:119] From /home/admin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.



Epoch 1/30
100/100 [==============================] - 25s 248ms/step - loss: 0.6896 - acc: 0.5345 - val_loss: 0.6734 - val_acc: 0.6180
Epoch 2/30
100/100 [==============================] - 23s 229ms/step - loss: 0.6606 - acc: 0.6075 - val_loss: 0.6485 - val_acc: 0.6390
Epoch 3/30
100/100 [==============================] - 23s 232ms/step - loss: 0.6200 - acc: 0.6760 - val_loss: 0.6345 - val_acc: 0.6390
Epoch 4/30
100/100 [==============================] - 33s 332ms/step - loss: 0.5678 - acc: 0.6980 - val_loss: 0.6068 - val_acc: 0.6640
Epoch 5/30
100/100 [==============================] - 42s 417ms/step - loss: 0.5401 - acc: 0.7175 - val_loss: 0.6245 - val_acc: 0.6590
Epoch 6/30
100/100 [==============================] - 40s 404ms/step - loss: 0.5125 - acc: 0.7320 - val_loss: 0.5895 - val_acc: 0.6930
Epoch 7/30
100/100 [==============================] - 32s 318ms/step - loss: 0.4854 - acc: 0.7660 - val_loss: 0.5774 - val_acc: 0.7010
Epoch 8/30
100/100 [==============================] - 22s 223ms/step - loss: 0.4511 - acc: 0.7945 - val_loss: 0.6084 - val_acc: 0.6900
Epoch 9/30
100/100 [==============================] - 22s 215ms/step - loss: 0.4255 - acc: 0.8060 - val_loss: 0.5546 - val_acc: 0.7220
Epoch 10/30
100/100 [==============================] - 23s 225ms/step - loss: 0.3978 - acc: 0.8225 - val_loss: 0.5706 - val_acc: 0.7140
Epoch 11/30
100/100 [==============================] - 22s 224ms/step - loss: 0.3811 - acc: 0.8365 - val_loss: 0.5561 - val_acc: 0.7140
Epoch 12/30
100/100 [==============================] - 24s 236ms/step - loss: 0.3492 - acc: 0.8415 - val_loss: 0.6049 - val_acc: 0.7030
Epoch 13/30
100/100 [==============================] - 22s 218ms/step - loss: 0.3304 - acc: 0.8585 - val_loss: 0.6377 - val_acc: 0.7120
Epoch 14/30
100/100 [==============================] - 22s 218ms/step - loss: 0.3090 - acc: 0.8725 - val_loss: 0.6054 - val_acc: 0.7310
Epoch 15/30
100/100 [==============================] - 22s 220ms/step - loss: 0.2827 - acc: 0.8855 - val_loss: 0.6269 - val_acc: 0.7220
Epoch 16/30
100/100 [==============================] - 22s 217ms/step - loss: 0.2586 - acc: 0.8990 - val_loss: 0.5976 - val_acc: 0.7360
Epoch 17/30
100/100 [==============================] - 22s 216ms/step - loss: 0.2405 - acc: 0.9020 - val_loss: 0.6068 - val_acc: 0.7360
Epoch 18/30
100/100 [==============================] - 22s 218ms/step - loss: 0.2243 - acc: 0.9150 - val_loss: 0.6273 - val_acc: 0.7220
Epoch 19/30
100/100 [==============================] - 22s 220ms/step - loss: 0.1974 - acc: 0.9270 - val_loss: 0.6847 - val_acc: 0.7200
Epoch 20/30
100/100 [==============================] - 22s 215ms/step - loss: 0.1717 - acc: 0.9355 - val_loss: 0.6677 - val_acc: 0.7270
Epoch 21/30
100/100 [==============================] - 22s 223ms/step - loss: 0.1549 - acc: 0.9485 - val_loss: 0.6992 - val_acc: 0.7300
Epoch 22/30
100/100 [==============================] - 22s 217ms/step - loss: 0.1447 - acc: 0.9535 - val_loss: 0.7294 - val_acc: 0.7240
Epoch 23/30
100/100 [==============================] - 22s 218ms/step - loss: 0.1247 - acc: 0.9595 - val_loss: 0.7609 - val_acc: 0.7240
Epoch 24/30
100/100 [==============================] - 22s 217ms/step - loss: 0.1073 - acc: 0.9645 - val_loss: 0.7735 - val_acc: 0.7230
Epoch 25/30
100/100 [==============================] - 22s 224ms/step - loss: 0.0948 - acc: 0.9660 - val_loss: 0.8297 - val_acc: 0.7230
Epoch 26/30
100/100 [==============================] - 22s 224ms/step - loss: 0.0802 - acc: 0.9750 - val_loss: 0.8406 - val_acc: 0.7110
Epoch 27/30
100/100 [==============================] - 22s 218ms/step - loss: 0.0752 - acc: 0.9765 - val_loss: 0.8831 - val_acc: 0.7260
Epoch 28/30
100/100 [==============================] - 22s 218ms/step - loss: 0.0572 - acc: 0.9890 - val_loss: 0.9550 - val_acc: 0.7200
Epoch 29/30
100/100 [==============================] - 22s 223ms/step - loss: 0.0487 - acc: 0.9880 - val_loss: 0.9380 - val_acc: 0.7190
Epoch 30/30
100/100 [==============================] - 22s 216ms/step - loss: 0.0458 - acc: 0.9890 - val_loss: 1.0508 - val_acc: 0.7190
```

<a name="4a81f33b"></a>
## 5.2.5 绘制损失和精度

```python
import matplotlib.pyplot as plt
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()
```



![output_14_0.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567760733495-f2a44ad6-d7fc-437b-bef2-a7656378f8af.png#align=left&display=inline&height=168&name=output_14_0.png&originHeight=264&originWidth=375&size=11411&status=done&width=239)![output_14_1.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567760733526-6d1fcfbc-26c4-4c64-88a8-e21603c43ddc.png#align=left&display=inline&height=169&name=output_14_1.png&originHeight=264&originWidth=375&size=12084&status=done&width=240)

<a name="75380285"></a>
## 5.2.6 数据增强

```
图像中都能看出过拟合的特征。训练精度随着时间线性增加，直到接近 100%，而验证精度则停留在 70%~72%。验证损失仅在 5 轮后就达到最小值，然后保持不变，
而训练损失则一直线性下降，直到接近于 0。因为训练样本相对较少（2000 个），所以过拟合是最关心的问题。前面已经介绍过几种降低过拟合的技巧，
比如 dropout 和权重衰减（L2 正则化）。现在将使用一种针对于计算机视觉领域的新方法，在用深度学习模型处理图像时几乎都会用到这种方法，它就是数据增强（data augmentation）。
---------------------------------------------------------------------------------------------------------------------------------------------
过拟合的原因是学习样本太少，导致无法训练出能够泛化到新数据的模型。如果拥有无限的数据，那么模型能够观察到数据分布的所有内容，这样就永远不会过拟合。
数据增强是从现有的训练样本中生成更多的训练数据，其方法是利用多种能够生成可信图像的随机变换来增加（augment）样本。其目标是，
模型在训练时不会两次查看完全相同的图像。这让模型能够观察到数据的更多内容，从而具有更好的泛化能力。
```

```python
datagen = ImageDataGenerator(rotation_range=40,
                             width_shift_range=0.2,
                             height_shift_range=0.2,
                             shear_range=0.2,
                             zoom_range=0.2,
                             horizontal_flip=True,
                            fill_mode='nearest')
```

```python
from keras.preprocessing import image
fnames = [os.path.join(train_dir_cat, fname) for fname in os.listdir(train_dir_cat)]
img_path = fnames[3]
img = image.load_img(img_path, target_size=(150, 150))
x = image.img_to_array(img)
x = x.reshape((1,) + x.shape)
i = 0
for batch in datagen.flow(x, batch_size=1):
    plt.figure(i)
    imgplot = plt.imshow(image.array_to_img(batch[0]))
    i += 1
    if i % 4 == 0:
        break
plt.show()
```


![output_17_0.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567760844347-2ea68096-0923-4184-94ee-58f9bc20ee7f.png#align=left&display=inline&height=157&name=output_17_0.png&originHeight=252&originWidth=261&size=60017&status=done&width=163)![output_17_1.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567760844378-1e3ada23-e1c4-4195-946e-72711205f8cd.png#align=left&display=inline&height=156&name=output_17_1.png&originHeight=252&originWidth=261&size=52772&status=done&width=162)![output_17_2.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567760844388-fbb25f76-cd79-46b7-b29c-a1eea7b4f8a7.png#align=left&display=inline&height=154&name=output_17_2.png&originHeight=252&originWidth=261&size=55814&status=done&width=160)![output_17_3.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567760844412-32853141-824c-4218-9180-ccb6dda55010.png#align=left&display=inline&height=160&name=output_17_3.png&originHeight=252&originWidth=261&size=56555&status=done&width=166)

如果使用这种数据增强来训练一个新网络，那么网络将不会两次看到同样的输入。但网络看到的输入仍然是高度相关的，因为这些输入都来自于少量的原始图像。无法生成新信息，<br />而只能混合现有信息。因此，这种方法可能不足以完全消除过拟合。为了进一步降低过拟合，还需要向模型中添加一个 Dropout 层，添加到密集连接分类器之前。

```python
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(lr=1e-4),metrics=['acc'])
```

```python
train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=40,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                     zoom_range=0.2,
                                     horizontal_flip=True,)

test_datagen = ImageDataGenerator(rescale=1./255)  # 验证数据是不能增强的  

train_generator = train_datagen.flow_from_directory(os.path.join(base_dir, 'train'),target_size=(150, 150),batch_size=32,class_mode='binary')
validation_generator = test_datagen.flow_from_directory(os.path.join(base_dir, 'validation'),target_size=(150, 150),batch_size=32,class_mode='binary')
history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=100,validation_data=validation_generator,validation_steps=50)
model.save('./models/cats_and_dogs_small_2.h5')
```

```
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/100
100/100 [==============================] - 58s 582ms/step - loss: 0.6933 - acc: 0.5178 - val_loss: 0.6925 - val_acc: 0.4962
Epoch 2/100
100/100 [==============================] - 52s 525ms/step - loss: 0.6843 - acc: 0.5634 - val_loss: 0.7200 - val_acc: 0.4987
Epoch 3/100
100/100 [==============================] - 53s 531ms/step - loss: 0.6730 - acc: 0.5922 - val_loss: 0.6554 - val_acc: 0.6180
Epoch 4/100
100/100 [==============================] - 53s 526ms/step - loss: 0.6589 - acc: 0.6038 - val_loss: 0.6553 - val_acc: 0.6012
Epoch 5/100
100/100 [==============================] - 53s 527ms/step - loss: 0.6415 - acc: 0.6244 - val_loss: 0.6113 - val_acc: 0.6586
Epoch 6/100
100/100 [==============================] - 53s 525ms/step - loss: 0.6333 - acc: 0.6344 - val_loss: 0.6398 - val_acc: 0.5973
Epoch 7/100
100/100 [==============================] - 57s 569ms/step - loss: 0.6051 - acc: 0.6741 - val_loss: 0.6134 - val_acc: 0.6523
Epoch 8/100
100/100 [==============================] - 60s 605ms/step - loss: 0.5972 - acc: 0.6759 - val_loss: 0.5774 - val_acc: 0.6849
Epoch 9/100
100/100 [==============================] - 58s 576ms/step - loss: 0.5974 - acc: 0.6725 - val_loss: 0.5724 - val_acc: 0.6888
Epoch 10/100
100/100 [==============================] - 53s 528ms/step - loss: 0.5821 - acc: 0.6937 - val_loss: 0.5495 - val_acc: 0.7195
Epoch 11/100
100/100 [==============================] - 53s 526ms/step - loss: 0.5813 - acc: 0.6994 - val_loss: 0.5741 - val_acc: 0.7036
Epoch 12/100
100/100 [==============================] - 53s 525ms/step - loss: 0.5824 - acc: 0.6928 - val_loss: 0.5452 - val_acc: 0.7183
Epoch 13/100
100/100 [==============================] - 53s 529ms/step - loss: 0.5686 - acc: 0.6941 - val_loss: 0.5645 - val_acc: 0.7004
Epoch 14/100
100/100 [==============================] - 53s 527ms/step - loss: 0.5710 - acc: 0.6913 - val_loss: 0.5928 - val_acc: 0.6758
Epoch 15/100
100/100 [==============================] - 53s 527ms/step - loss: 0.5509 - acc: 0.7138 - val_loss: 0.5637 - val_acc: 0.7030
Epoch 16/100
100/100 [==============================] - 53s 528ms/step - loss: 0.5524 - acc: 0.7059 - val_loss: 0.5229 - val_acc: 0.7397
Epoch 17/100
100/100 [==============================] - 53s 525ms/step - loss: 0.5458 - acc: 0.7156 - val_loss: 0.5046 - val_acc: 0.7500
Epoch 18/100
100/100 [==============================] - 53s 528ms/step - loss: 0.5460 - acc: 0.7203 - val_loss: 0.5496 - val_acc: 0.7120
Epoch 19/100
100/100 [==============================] - 52s 524ms/step - loss: 0.5341 - acc: 0.7247 - val_loss: 0.5551 - val_acc: 0.7208
Epoch 20/100
100/100 [==============================] - 53s 527ms/step - loss: 0.5356 - acc: 0.7356 - val_loss: 0.5081 - val_acc: 0.7294
Epoch 21/100
100/100 [==============================] - 53s 526ms/step - loss: 0.5277 - acc: 0.7313 - val_loss: 0.5068 - val_acc: 0.7341
Epoch 22/100
100/100 [==============================] - 53s 526ms/step - loss: 0.5220 - acc: 0.7419 - val_loss: 0.5349 - val_acc: 0.7229
Epoch 23/100
100/100 [==============================] - 53s 527ms/step - loss: 0.5185 - acc: 0.7437 - val_loss: 0.5070 - val_acc: 0.7487
Epoch 24/100
100/100 [==============================] - 53s 525ms/step - loss: 0.5305 - acc: 0.7381 - val_loss: 0.5453 - val_acc: 0.7113
Epoch 25/100
100/100 [==============================] - 53s 527ms/step - loss: 0.5138 - acc: 0.7513 - val_loss: 0.4758 - val_acc: 0.7719
Epoch 26/100
100/100 [==============================] - 53s 528ms/step - loss: 0.5151 - acc: 0.7425 - val_loss: 0.4890 - val_acc: 0.7468
Epoch 27/100
100/100 [==============================] - 53s 528ms/step - loss: 0.5024 - acc: 0.7519 - val_loss: 0.5744 - val_acc: 0.7268
Epoch 28/100
100/100 [==============================] - 53s 526ms/step - loss: 0.5054 - acc: 0.7541 - val_loss: 0.5247 - val_acc: 0.7322
Epoch 29/100
100/100 [==============================] - 53s 526ms/step - loss: 0.4911 - acc: 0.7625 - val_loss: 0.4875 - val_acc: 0.7661
Epoch 30/100
100/100 [==============================] - 53s 531ms/step - loss: 0.4934 - acc: 0.7556 - val_loss: 0.6100 - val_acc: 0.6891
Epoch 31/100
100/100 [==============================] - 53s 527ms/step - loss: 0.4879 - acc: 0.7697 - val_loss: 0.5183 - val_acc: 0.7584
Epoch 32/100
100/100 [==============================] - 57s 570ms/step - loss: 0.4848 - acc: 0.7584 - val_loss: 0.4732 - val_acc: 0.7661
Epoch 33/100
100/100 [==============================] - 60s 603ms/step - loss: 0.4740 - acc: 0.7716 - val_loss: 0.5286 - val_acc: 0.7392
Epoch 34/100
100/100 [==============================] - 57s 573ms/step - loss: 0.4806 - acc: 0.7750 - val_loss: 0.4579 - val_acc: 0.7912
Epoch 35/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4783 - acc: 0.7703 - val_loss: 0.4846 - val_acc: 0.7602
Epoch 36/100
100/100 [==============================] - 53s 525ms/step - loss: 0.4727 - acc: 0.7803 - val_loss: 0.4977 - val_acc: 0.7545
Epoch 37/100
100/100 [==============================] - 53s 530ms/step - loss: 0.4716 - acc: 0.7772 - val_loss: 0.4671 - val_acc: 0.7817
Epoch 38/100
100/100 [==============================] - 52s 524ms/step - loss: 0.4673 - acc: 0.7766 - val_loss: 0.4676 - val_acc: 0.7919
Epoch 39/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4659 - acc: 0.7788 - val_loss: 0.5002 - val_acc: 0.7659
Epoch 40/100
100/100 [==============================] - 53s 527ms/step - loss: 0.4589 - acc: 0.7835 - val_loss: 0.4549 - val_acc: 0.7758
Epoch 41/100
100/100 [==============================] - 53s 531ms/step - loss: 0.4632 - acc: 0.7841 - val_loss: 0.6468 - val_acc: 0.7036
Epoch 42/100
100/100 [==============================] - 53s 531ms/step - loss: 0.4579 - acc: 0.7831 - val_loss: 0.4993 - val_acc: 0.7621
Epoch 43/100
100/100 [==============================] - 52s 525ms/step - loss: 0.4690 - acc: 0.7762 - val_loss: 0.4552 - val_acc: 0.7758
Epoch 44/100
100/100 [==============================] - 53s 530ms/step - loss: 0.4628 - acc: 0.7763 - val_loss: 0.4739 - val_acc: 0.7671
Epoch 45/100
100/100 [==============================] - 53s 527ms/step - loss: 0.4499 - acc: 0.7891 - val_loss: 0.5789 - val_acc: 0.7133
Epoch 46/100
100/100 [==============================] - 53s 529ms/step - loss: 0.4464 - acc: 0.7894 - val_loss: 0.4564 - val_acc: 0.7925
Epoch 47/100
100/100 [==============================] - 52s 525ms/step - loss: 0.4566 - acc: 0.7869 - val_loss: 0.4669 - val_acc: 0.7919
Epoch 48/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4484 - acc: 0.7822 - val_loss: 0.4539 - val_acc: 0.7945
Epoch 49/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4455 - acc: 0.7963 - val_loss: 0.5045 - val_acc: 0.7614
Epoch 50/100
100/100 [==============================] - 53s 527ms/step - loss: 0.4395 - acc: 0.7869 - val_loss: 0.5074 - val_acc: 0.7539
Epoch 51/100
100/100 [==============================] - 53s 530ms/step - loss: 0.4255 - acc: 0.7997 - val_loss: 0.4387 - val_acc: 0.7938
Epoch 52/100
100/100 [==============================] - 53s 529ms/step - loss: 0.4252 - acc: 0.8066 - val_loss: 0.4374 - val_acc: 0.8067
Epoch 53/100
100/100 [==============================] - 53s 529ms/step - loss: 0.4349 - acc: 0.7944 - val_loss: 0.5005 - val_acc: 0.7792
Epoch 54/100
100/100 [==============================] - 53s 527ms/step - loss: 0.4371 - acc: 0.7956 - val_loss: 0.4374 - val_acc: 0.8054
Epoch 55/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4239 - acc: 0.8031 - val_loss: 0.4969 - val_acc: 0.7659
Epoch 56/100
100/100 [==============================] - 53s 531ms/step - loss: 0.4231 - acc: 0.8022 - val_loss: 0.5004 - val_acc: 0.7577
Epoch 57/100
100/100 [==============================] - 57s 571ms/step - loss: 0.4239 - acc: 0.8128 - val_loss: 0.4438 - val_acc: 0.7925
Epoch 58/100
100/100 [==============================] - 61s 605ms/step - loss: 0.4342 - acc: 0.7966 - val_loss: 0.4356 - val_acc: 0.7970
Epoch 59/100
100/100 [==============================] - 58s 580ms/step - loss: 0.4211 - acc: 0.8091 - val_loss: 0.4556 - val_acc: 0.7932
Epoch 60/100
100/100 [==============================] - 53s 527ms/step - loss: 0.4083 - acc: 0.8128 - val_loss: 0.4527 - val_acc: 0.7906
Epoch 61/100
100/100 [==============================] - 53s 529ms/step - loss: 0.4130 - acc: 0.8072 - val_loss: 0.5394 - val_acc: 0.7442
Epoch 62/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4105 - acc: 0.8059 - val_loss: 0.5327 - val_acc: 0.7538
Epoch 63/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4016 - acc: 0.8169 - val_loss: 0.4962 - val_acc: 0.7622
Epoch 64/100
100/100 [==============================] - 53s 527ms/step - loss: 0.4101 - acc: 0.8125 - val_loss: 0.4498 - val_acc: 0.7996
Epoch 65/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4069 - acc: 0.8138 - val_loss: 0.4379 - val_acc: 0.7989
Epoch 66/100
100/100 [==============================] - 53s 527ms/step - loss: 0.4060 - acc: 0.8203 - val_loss: 0.4405 - val_acc: 0.8086
Epoch 67/100
100/100 [==============================] - 53s 528ms/step - loss: 0.4011 - acc: 0.8225 - val_loss: 0.5500 - val_acc: 0.7589
Epoch 68/100
100/100 [==============================] - 53s 525ms/step - loss: 0.4005 - acc: 0.8175 - val_loss: 0.4891 - val_acc: 0.7777
Epoch 69/100
100/100 [==============================] - 53s 534ms/step - loss: 0.4070 - acc: 0.8234 - val_loss: 0.5002 - val_acc: 0.7690
Epoch 70/100
100/100 [==============================] - 53s 533ms/step - loss: 0.3938 - acc: 0.8216 - val_loss: 0.4234 - val_acc: 0.8177
Epoch 71/100
100/100 [==============================] - 54s 538ms/step - loss: 0.3793 - acc: 0.8294 - val_loss: 0.4580 - val_acc: 0.8020
Epoch 72/100
100/100 [==============================] - 53s 531ms/step - loss: 0.3875 - acc: 0.8312 - val_loss: 0.4876 - val_acc: 0.7880
Epoch 73/100
100/100 [==============================] - 53s 527ms/step - loss: 0.3791 - acc: 0.8216 - val_loss: 0.4158 - val_acc: 0.8189
Epoch 74/100
100/100 [==============================] - 53s 531ms/step - loss: 0.3875 - acc: 0.8256 - val_loss: 0.4232 - val_acc: 0.8090
Epoch 75/100
100/100 [==============================] - 53s 526ms/step - loss: 0.3823 - acc: 0.8253 - val_loss: 0.4590 - val_acc: 0.7841
Epoch 76/100
100/100 [==============================] - 53s 530ms/step - loss: 0.3782 - acc: 0.8375 - val_loss: 0.4274 - val_acc: 0.8198
Epoch 77/100
100/100 [==============================] - 53s 525ms/step - loss: 0.3804 - acc: 0.8300 - val_loss: 0.4304 - val_acc: 0.8196
Epoch 78/100
100/100 [==============================] - 53s 528ms/step - loss: 0.3837 - acc: 0.8294 - val_loss: 0.4186 - val_acc: 0.8204
Epoch 79/100
100/100 [==============================] - 53s 529ms/step - loss: 0.3649 - acc: 0.8441 - val_loss: 0.4453 - val_acc: 0.7990
Epoch 80/100
100/100 [==============================] - 53s 529ms/step - loss: 0.3677 - acc: 0.8387 - val_loss: 0.4243 - val_acc: 0.8099
Epoch 81/100
100/100 [==============================] - 53s 526ms/step - loss: 0.3763 - acc: 0.8378 - val_loss: 0.4277 - val_acc: 0.8077
Epoch 82/100
100/100 [==============================] - 57s 569ms/step - loss: 0.3692 - acc: 0.8384 - val_loss: 0.4759 - val_acc: 0.8099
Epoch 83/100
100/100 [==============================] - 60s 603ms/step - loss: 0.3607 - acc: 0.8422 - val_loss: 0.4240 - val_acc: 0.8179
Epoch 84/100
100/100 [==============================] - 58s 576ms/step - loss: 0.3636 - acc: 0.8381 - val_loss: 0.4273 - val_acc: 0.8215
Epoch 85/100
100/100 [==============================] - 53s 527ms/step - loss: 0.3568 - acc: 0.8444 - val_loss: 0.4194 - val_acc: 0.8217
Epoch 86/100
100/100 [==============================] - 53s 530ms/step - loss: 0.3563 - acc: 0.8416 - val_loss: 0.4539 - val_acc: 0.8144
Epoch 87/100
100/100 [==============================] - 53s 529ms/step - loss: 0.3490 - acc: 0.8481 - val_loss: 0.4255 - val_acc: 0.8166
Epoch 88/100
100/100 [==============================] - 53s 529ms/step - loss: 0.3699 - acc: 0.8272 - val_loss: 0.4336 - val_acc: 0.8164
Epoch 89/100
100/100 [==============================] - 53s 529ms/step - loss: 0.3519 - acc: 0.8475 - val_loss: 0.3846 - val_acc: 0.8247
Epoch 90/100
100/100 [==============================] - 53s 531ms/step - loss: 0.3458 - acc: 0.8409 - val_loss: 0.4500 - val_acc: 0.8128
Epoch 91/100
100/100 [==============================] - 54s 539ms/step - loss: 0.3343 - acc: 0.8547 - val_loss: 0.4626 - val_acc: 0.8177
Epoch 92/100
100/100 [==============================] - 53s 526ms/step - loss: 0.3506 - acc: 0.8481 - val_loss: 0.4411 - val_acc: 0.8198
Epoch 93/100
100/100 [==============================] - 53s 530ms/step - loss: 0.3513 - acc: 0.8475 - val_loss: 0.4289 - val_acc: 0.8202
Epoch 94/100
100/100 [==============================] - 53s 530ms/step - loss: 0.3449 - acc: 0.8469 - val_loss: 0.4294 - val_acc: 0.8122
Epoch 95/100
100/100 [==============================] - 53s 529ms/step - loss: 0.3523 - acc: 0.8444 - val_loss: 0.4196 - val_acc: 0.8312
Epoch 96/100
100/100 [==============================] - 53s 526ms/step - loss: 0.3405 - acc: 0.8497 - val_loss: 0.4812 - val_acc: 0.8048
Epoch 97/100
100/100 [==============================] - 53s 528ms/step - loss: 0.3213 - acc: 0.8575 - val_loss: 0.4529 - val_acc: 0.8096
Epoch 98/100
100/100 [==============================] - 53s 531ms/step - loss: 0.3425 - acc: 0.8488 - val_loss: 0.4563 - val_acc: 0.8028
Epoch 99/100
100/100 [==============================] - 53s 535ms/step - loss: 0.3379 - acc: 0.8581 - val_loss: 0.4233 - val_acc: 0.8160
Epoch 100/100
100/100 [==============================] - 54s 535ms/step - loss: 0.3334 - acc: 0.8541 - val_loss: 0.4358 - val_acc: 0.8286
```

```python
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()
```


![output_21_0.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567760955688-b5d30630-0023-408d-b97a-4d359342fda6.png#align=left&display=inline&height=192&name=output_21_0.png&originHeight=264&originWidth=381&size=18295&status=done&width=276)![output_21_1.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567760955724-186707bd-211a-40d5-89f9-15e6174c24bf.png#align=left&display=inline&height=193&name=output_21_1.png&originHeight=264&originWidth=381&size=20382&status=done&width=279)

<a name="6a071462"></a>
# 5.3 使用预训练的卷积神经网络

<a name="VGG16"></a>
## VGG16

```python
from keras.applications import VGG16
conv_base = VGG16(weights='imagenet',include_top=False,input_shape=(150, 150, 3))
# weights 指定模型初始化的权重检查点。
# include_top 指定模型最后是否包含密集连接分类器。默认情况下，这个密集连接分类器对应于 ImageNet 的 1000 个类别。因为我们打算使用自己的密集连接分类器（只有两个类别：cat 和 dog），所以不需要包含它。
# input_shape 是输入到网络中的图像张量的形状。这个参数完全是可选的，如果不传入这个参数，那么网络能够处理任意形状的输入。
```

```
Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5
58892288/58889256 [==============================] - 70s 1us/step
```

```python
conv_base.summary()
```

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
```

```python
import os
import numpy as np
train_dir =  os.path.join(base_dir, 'train')
validation_dir =  os.path.join(base_dir, 'validation')
test_dir =  os.path.join(base_dir, 'test')
from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)
batch_size = 20
def extract_features(directory, sample_count):
    features = np.zeros(shape=(sample_count, 4, 4, 512))
    labels = np.zeros(shape=(sample_count))
    generator = datagen.flow_from_directory(directory,target_size=(150, 150),batch_size=batch_size,class_mode='binary')
    i = 0
    for inputs_batch, labels_batch in generator:
        features_batch = conv_base.predict(inputs_batch)
        features[i * batch_size : (i + 1) * batch_size] = features_batch
        labels[i * batch_size : (i + 1) * batch_size] = labels_batch
        i += 1
        if i * batch_size >= sample_count:
             break
    return features, labels
train_features, train_labels = extract_features(train_dir, 2000)
validation_features, validation_labels = extract_features(validation_dir, 1000)
test_features, test_labels = extract_features(test_dir, 1000)
```

```
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
```

```python
train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
test_features = np.reshape(test_features, (1000, 4 * 4 * 512))
```

```python
from keras import models
from keras import layers
from keras import optimizers
model = models.Sequential()
model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer=optimizers.RMSprop(lr=2e-5),loss='binary_crossentropy',metrics=['acc'])
history = model.fit(train_features, train_labels,epochs=30,batch_size=20,validation_data=(validation_features, validation_labels))
```

```
Train on 2000 samples, validate on 1000 samples
Epoch 1/30
2000/2000 [==============================] - 10s 5ms/step - loss: 0.5931 - acc: 0.6665 - val_loss: 0.4499 - val_acc: 0.8130
Epoch 2/30
2000/2000 [==============================] - 10s 5ms/step - loss: 0.4276 - acc: 0.8135 - val_loss: 0.3621 - val_acc: 0.8690
Epoch 3/30
2000/2000 [==============================] - 9s 4ms/step - loss: 0.3503 - acc: 0.8555 - val_loss: 0.3185 - val_acc: 0.8780
Epoch 4/30
2000/2000 [==============================] - 9s 5ms/step - loss: 0.3111 - acc: 0.8760 - val_loss: 0.2950 - val_acc: 0.8870
Epoch 5/30
2000/2000 [==============================] - 10s 5ms/step - loss: 0.2799 - acc: 0.8930 - val_loss: 0.2832 - val_acc: 0.8840
Epoch 6/30
2000/2000 [==============================] - 8s 4ms/step - loss: 0.2610 - acc: 0.8935 - val_loss: 0.2700 - val_acc: 0.8850
Epoch 7/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.2498 - acc: 0.8995 - val_loss: 0.2653 - val_acc: 0.8900
Epoch 8/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.2317 - acc: 0.9105 - val_loss: 0.2656 - val_acc: 0.8900
Epoch 9/30
2000/2000 [==============================] - 6s 3ms/step - loss: 0.2148 - acc: 0.9225 - val_loss: 0.2560 - val_acc: 0.8920
Epoch 10/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.2066 - acc: 0.9235 - val_loss: 0.2619 - val_acc: 0.8920
Epoch 11/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1986 - acc: 0.9270 - val_loss: 0.2565 - val_acc: 0.8950
Epoch 12/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1853 - acc: 0.9330 - val_loss: 0.2518 - val_acc: 0.8960
Epoch 13/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1824 - acc: 0.9335 - val_loss: 0.2564 - val_acc: 0.8970
Epoch 14/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1746 - acc: 0.9345 - val_loss: 0.2430 - val_acc: 0.8950
Epoch 15/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1569 - acc: 0.9450 - val_loss: 0.2385 - val_acc: 0.9020
Epoch 16/30
2000/2000 [==============================] - 6s 3ms/step - loss: 0.1562 - acc: 0.9440 - val_loss: 0.2371 - val_acc: 0.9010
Epoch 17/30
2000/2000 [==============================] - 6s 3ms/step - loss: 0.1527 - acc: 0.9490 - val_loss: 0.2376 - val_acc: 0.9020
Epoch 18/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1453 - acc: 0.9445 - val_loss: 0.2375 - val_acc: 0.9010
Epoch 19/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1328 - acc: 0.9560 - val_loss: 0.2369 - val_acc: 0.9000
Epoch 20/30
2000/2000 [==============================] - 7s 4ms/step - loss: 0.1303 - acc: 0.9520 - val_loss: 0.2411 - val_acc: 0.9020
Epoch 21/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1279 - acc: 0.9555 - val_loss: 0.2391 - val_acc: 0.9000
Epoch 22/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1197 - acc: 0.9645 - val_loss: 0.2383 - val_acc: 0.9020
Epoch 23/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1151 - acc: 0.9620 - val_loss: 0.2449 - val_acc: 0.9020
Epoch 24/30
2000/2000 [==============================] - 6s 3ms/step - loss: 0.1069 - acc: 0.9640 - val_loss: 0.2388 - val_acc: 0.9020
Epoch 25/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1089 - acc: 0.9615 - val_loss: 0.2613 - val_acc: 0.8920
Epoch 26/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1037 - acc: 0.9635 - val_loss: 0.2405 - val_acc: 0.8990
Epoch 27/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.0986 - acc: 0.9665 - val_loss: 0.2374 - val_acc: 0.9020
Epoch 28/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.1025 - acc: 0.9660 - val_loss: 0.2468 - val_acc: 0.9000
Epoch 29/30
2000/2000 [==============================] - 7s 3ms/step - loss: 0.0908 - acc: 0.9695 - val_loss: 0.2408 - val_acc: 0.9010
Epoch 30/30
2000/2000 [==============================] - 7s 4ms/step - loss: 0.0839 - acc: 0.9745 - val_loss: 0.2503 - val_acc: 0.9010
```

```python
import matplotlib.pyplot as plt
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()
```

![output_29_0.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567761014138-e07283d6-4b37-4558-b6e4-9d6d51efcb5b.png#align=left&display=inline&height=137&name=output_29_0.png&originHeight=264&originWidth=381&size=11261&status=done&width=198)![output_29_1.png](https://cdn.nlark.com/yuque/0/2019/png/280095/1567761014195-ff1a6ea4-7e3e-4936-a773-52d2ed981791.png#align=left&display=inline&height=141&name=output_29_1.png&originHeight=264&originWidth=375&size=11399&status=done&width=200)



<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />


